#!/usr/bin/perl -w
# $Id$
use strict;
require v5.8.0;
our $VERSION = 'v0.3';

BEGIN
{
    my $PROGRAM_DIR = $0;
    $PROGRAM_DIR =~ s/[^\/\\]+$//;
    $PROGRAM_DIR = "./" unless($PROGRAM_DIR);
    unshift @INC, 
        map "$PROGRAM_DIR$_",qw{modules lib ../modules ..lib};
}

use MyPlace::Program::Batchget;
my $self = new MyPlace::Program::Batchget;
$self->set(@ARGV);
if(! $self->{tasks} or @{$self->{tasks}} < 1) {
	$self->readfile(\*STDIN);
}
exit $self->execute;



__END__

=pod

=head1  NAME

batchget - A batch mode downloader

=head1  SYNOPSIS

batchget [options] ...

cat url.lst | batchget

cat url.lst | batchget -a -d 

=head1  OPTIONS

=over 12

=item B<-a,--autoname>

Use indexing of URLs as output filename 

=item B<-b,--cookie>

Use cookie jar

=item B<-c,--nc,--no-clobber>

No clobber when target exists.

=item B<-d,--directory>

Create directories

=item B<-e,--ext>

Extension name for autonaming

=item B<-f,--fullname>

Use URL as output filename

=item B<-i,--numlen>

Number length for index filename

=item B<-M,--maxtime>

Max time for a single download process

=item B<-m,--maxtask>

Max number of simulatanous downloading task

=item B<-n,--taskname>

Task name

=item B<-r,--referer>

Global referer URL

=item B<-w,--workdir>

Global working directory

=item B<-U,--urlhist>

Use URL downloading history databasa

=item B<--version>

Print version infomation.

=item B<-h>,B<--help>

Print a brief help message and exits.

=item B<--manual>,B<--man>

View application manual

=item B<--edit-me>

Invoke 'editor' against the source

=back

=head1  DESCRIPTION

A downloader which can download multiple urls at the same time and/or in queue.

=head1  CHANGELOG

    2007-10-28  xiaoranzzz  <xiaoranzzz@myplace.hell>
    
        * file created, version 0.1

    2010-08-03  xiaoranzzz  <xiaoranzzz@myplace.hell>
        
        * update to version 0.2

=head1  AUTHOR

xiaoranzzz <xiaoranzzz@myplace.hell>

=cut


